{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONLozz/0T+YDB4WNNdiqAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kravis1216/aws-eks-kubernetes-masterclass/blob/master/1_8_caching_llm_response.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-Memory Cache"
      ],
      "metadata": {
        "id": "ZzSBJPDEoy98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain_openai"
      ],
      "metadata": {
        "id": "zJ1dyrd6pMJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "6ICsoTX-ozs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# To make the caching really obvious, lets use a slower model.\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)"
      ],
      "metadata": {
        "id": "iY_RMBZzprIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "prompt = \"Tell me about applet.\"\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "I_JOSe32p1mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "7WKxSv3xrEbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQLite Caching"
      ],
      "metadata": {
        "id": "r9fTmnjrsqnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
      ],
      "metadata": {
        "id": "i7aWL6YPstAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # First request (not in cache, takes longer)\n",
        " %%time\n",
        " llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "nFNvNMJstL8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # First request (cached, faster)\n",
        " %%time\n",
        " llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "5jOIE1bFtbxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "X7MkZncdthRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2uu9LeKuB24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}